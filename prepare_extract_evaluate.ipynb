{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c039144f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ilya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ilya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4227a3b",
   "metadata": {},
   "source": [
    "## Подготовка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff1af3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B057586722045</td>\n",
       "      <td>_ЁЭффективное численное решение задачи Стокса ...</td>\n",
       "      <td>_ЁРассматривается задача Стокса с разрывным ко...</td>\n",
       "      <td>_ЁХабаровск\\_Ёзадача Стокса с разрывным коэффи...</td>\n",
       "      <td>e8\\f2</td>\n",
       "      <td>13Д\\16Б</td>\n",
       "      <td>27.41.19\\30.17.02</td>\n",
       "      <td>271.41.19.17.21\\301.17.02.11</td>\n",
       "      <td>###</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B10213980294</td>\n",
       "      <td>_ЁАвтоматическая маршрутизация в среде с препя...</td>\n",
       "      <td>_ЁВ работе рассматривается перемещение тела гр...</td>\n",
       "      <td>_Ёавтоматическая маршрутизация\\_Ёинтеллектуаль...</td>\n",
       "      <td>e8\\e1\\e9\\00</td>\n",
       "      <td>93\\93-Y\\37\\37-Y\\РС\\81</td>\n",
       "      <td>28.23.27\\50.41.21\\55.30.31</td>\n",
       "      <td>282.23.27.09\\509.41.21.25.19\\551.30.31.07.13</td>\n",
       "      <td>###</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B103249601493</td>\n",
       "      <td>Структурно-динамический анализ с помощью ЯМР-р...</td>\n",
       "      <td>Изучение движения молекул и атомов в твердых т...</td>\n",
       "      <td>Татарстан\\ЯМР-релаксометрия\\газы природные\\доб...</td>\n",
       "      <td>e5\\f7</td>\n",
       "      <td>08Д\\19А</td>\n",
       "      <td>31.01.33\\38.57.23</td>\n",
       "      <td>311.01.33\\383.57.23.01</td>\n",
       "      <td>###</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B10349920193X</td>\n",
       "      <td>Структурно-функциональные исследования пептидо...</td>\n",
       "      <td>Излагается разрабатываемая концепция создания ...</td>\n",
       "      <td>изучение\\лекарственные средства\\пептиды\\разраб...</td>\n",
       "      <td>e3</td>\n",
       "      <td>04Р1\\04Б3</td>\n",
       "      <td>62.13.39</td>\n",
       "      <td>621.13.39</td>\n",
       "      <td>###</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B103499202650</td>\n",
       "      <td>Применение хроматографии и масс-спектрометрии ...</td>\n",
       "      <td>В результате проведенных исследований разработ...</td>\n",
       "      <td>Galleria mellonella\\антибактериальная активнос...</td>\n",
       "      <td>e3</td>\n",
       "      <td>04Р1\\04Б3</td>\n",
       "      <td>62.13.39</td>\n",
       "      <td>621.13.39</td>\n",
       "      <td>###</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0                                                  1  \\\n",
       "0  B057586722045  _ЁЭффективное численное решение задачи Стокса ...   \n",
       "1   B10213980294  _ЁАвтоматическая маршрутизация в среде с препя...   \n",
       "2  B103249601493  Структурно-динамический анализ с помощью ЯМР-р...   \n",
       "3  B10349920193X  Структурно-функциональные исследования пептидо...   \n",
       "4  B103499202650  Применение хроматографии и масс-спектрометрии ...   \n",
       "\n",
       "                                                   2  \\\n",
       "0  _ЁРассматривается задача Стокса с разрывным ко...   \n",
       "1  _ЁВ работе рассматривается перемещение тела гр...   \n",
       "2  Изучение движения молекул и атомов в твердых т...   \n",
       "3  Излагается разрабатываемая концепция создания ...   \n",
       "4  В результате проведенных исследований разработ...   \n",
       "\n",
       "                                                   3            4  \\\n",
       "0  _ЁХабаровск\\_Ёзадача Стокса с разрывным коэффи...        e8\\f2   \n",
       "1  _Ёавтоматическая маршрутизация\\_Ёинтеллектуаль...  e8\\e1\\e9\\00   \n",
       "2  Татарстан\\ЯМР-релаксометрия\\газы природные\\доб...        e5\\f7   \n",
       "3  изучение\\лекарственные средства\\пептиды\\разраб...           e3   \n",
       "4  Galleria mellonella\\антибактериальная активнос...           e3   \n",
       "\n",
       "                       5                           6  \\\n",
       "0                13Д\\16Б           27.41.19\\30.17.02   \n",
       "1  93\\93-Y\\37\\37-Y\\РС\\81  28.23.27\\50.41.21\\55.30.31   \n",
       "2                08Д\\19А           31.01.33\\38.57.23   \n",
       "3              04Р1\\04Б3                    62.13.39   \n",
       "4              04Р1\\04Б3                    62.13.39   \n",
       "\n",
       "                                              7    8  \n",
       "0                  271.41.19.17.21\\301.17.02.11  ###  \n",
       "1  282.23.27.09\\509.41.21.25.19\\551.30.31.07.13  ###  \n",
       "2                        311.01.33\\383.57.23.01  ###  \n",
       "3                                     621.13.39  ###  \n",
       "4                                     621.13.39  ###  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('dataset1.csv', encoding='cp1251', sep='\\t', header=None)\n",
    "df2 = pd.read_csv('dataset2.csv', encoding='cp1251', sep='\\t', header=None, quoting=3)\n",
    "df = pd.concat([df1, df2], axis=0).reset_index()\n",
    "del df['index']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "523d606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[:50].to_csv('dataset1_example.csv', index=False)\n",
    "df2[:50].to_csv('dataset2_example.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c709d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open('viniti_md.txt','r',encoding='cp1251',)\n",
    "tx = txt.readlines()\n",
    "alphabet = []\n",
    "for x in tx:\n",
    "    alphabet.append(x.strip('\\n').split('\\t'))\n",
    "    \n",
    "alpha_df = pd.DataFrame(alphabet, columns=['символ',\"значение\", \"код\", \"категория\"])\n",
    "\n",
    "def valid_alpha(grp:list) -> list:\n",
    "    unvalid_alpha = alpha_df.copy()\n",
    "    unvalid_alpha.drop(unvalid_alpha.index[unvalid_alpha['категория'].isin(grp)], inplace=True)\n",
    "    return sorted(unvalid_alpha['символ'], key=lambda x: len(x), reverse=True)\n",
    "\n",
    "unvalid_alpha = valid_alpha(['буквы лат.','цифры','буквы рус.', 'пробел'])\n",
    "for allowed_el in ['-', '.', '!', ',', ':', ';', '?']:\n",
    "    unvalid_alpha.remove(allowed_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79fa6504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(unvalid_alpha: list, s: str)->str:\n",
    "    current_str = s\n",
    "    for x in unvalid_alpha:\n",
    "        current_str = current_str.replace(x, ' ')\n",
    "    tokens = current_str.split(' ')\n",
    "    current_str = ' '.join([token for token in tokens if token != ''])\n",
    "        \n",
    "    return current_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b3739b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keywords</th>\n",
       "      <th>name</th>\n",
       "      <th>anot</th>\n",
       "      <th>name_anot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Хабаровск, задача Стокса с разрывным коэффици...</td>\n",
       "      <td>Эффективное численное решение задачи Стокса с ...</td>\n",
       "      <td>Рассматривается задача Стокса с разрывным коэф...</td>\n",
       "      <td>Эффективное численное решение задачи Стокса с ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[автоматическая маршрутизация, интеллектуальны...</td>\n",
       "      <td>Автоматическая маршрутизация в среде с препятс...</td>\n",
       "      <td>В работе рассматривается перемещение тела груп...</td>\n",
       "      <td>Автоматическая маршрутизация в среде с препятс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Татарстан, ЯМР-релаксометрия, газы природные,...</td>\n",
       "      <td>Структурно-динамический анализ с помощью ЯМР-р...</td>\n",
       "      <td>Изучение движения молекул и атомов в твердых т...</td>\n",
       "      <td>Структурно-динамический анализ с помощью ЯМР-р...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[изучение, лекарственные средства, пептиды, ра...</td>\n",
       "      <td>Структурно-функциональные исследования пептидо...</td>\n",
       "      <td>Излагается разрабатываемая концепция создания ...</td>\n",
       "      <td>Структурно-функциональные исследования пептидо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Galleria mellonella, антибактериальная активн...</td>\n",
       "      <td>Применение хроматографии и масс-спектрометрии ...</td>\n",
       "      <td>В результате проведенных исследований разработ...</td>\n",
       "      <td>Применение хроматографии и масс-спектрометрии ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            keywords  \\\n",
       "0  [Хабаровск, задача Стокса с разрывным коэффици...   \n",
       "1  [автоматическая маршрутизация, интеллектуальны...   \n",
       "2  [Татарстан, ЯМР-релаксометрия, газы природные,...   \n",
       "3  [изучение, лекарственные средства, пептиды, ра...   \n",
       "4  [Galleria mellonella, антибактериальная активн...   \n",
       "\n",
       "                                                name  \\\n",
       "0  Эффективное численное решение задачи Стокса с ...   \n",
       "1  Автоматическая маршрутизация в среде с препятс...   \n",
       "2  Структурно-динамический анализ с помощью ЯМР-р...   \n",
       "3  Структурно-функциональные исследования пептидо...   \n",
       "4  Применение хроматографии и масс-спектрометрии ...   \n",
       "\n",
       "                                                anot  \\\n",
       "0  Рассматривается задача Стокса с разрывным коэф...   \n",
       "1  В работе рассматривается перемещение тела груп...   \n",
       "2  Изучение движения молекул и атомов в твердых т...   \n",
       "3  Излагается разрабатываемая концепция создания ...   \n",
       "4  В результате проведенных исследований разработ...   \n",
       "\n",
       "                                           name_anot  \n",
       "0  Эффективное численное решение задачи Стокса с ...  \n",
       "1  Автоматическая маршрутизация в среде с препятс...  \n",
       "2  Структурно-динамический анализ с помощью ЯМР-р...  \n",
       "3  Структурно-функциональные исследования пептидо...  \n",
       "4  Применение хроматографии и масс-спектрометрии ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "name = 1\n",
    "anot = 2\n",
    "keywords = 3\n",
    "\n",
    "df['keywords'] = df[keywords].apply(lambda x: list(map(lambda y: text_processing(unvalid_alpha, y), x.split('\\\\'))))\n",
    "df['name'] = df[name].apply(lambda x: text_processing(unvalid_alpha, x))\n",
    "df['anot'] = df[anot].apply(lambda x: text_processing(unvalid_alpha, x))\n",
    "\n",
    "df_cleared = df[['keywords', 'name', 'anot']]\n",
    "df_cleared['name_anot'] = df_cleared.apply(lambda x: x[1]+'. '+ x[2], axis=1)\n",
    "df_cleared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f98ba4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выборка из 10тысяч, по которым происходил замер качества BERT-моделей\n",
    "df_exp = df_cleared.sample(10000, random_state=42).reset_index()\n",
    "df_exp.to_csv('df_exp.csv', index=False)\n",
    "\n",
    "# Выборка из 50 сэмплов, для использования в данном ноутбуке\n",
    "df_exp_short = df_cleared.sample(50, random_state=42).reset_index()\n",
    "df_exp_short.to_csv('df_exp_short.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f60380",
   "metadata": {},
   "source": [
    "## Получение ключевых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2b193c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords_list(add_external=True):\n",
    "    nltk_list = nltk.corpus.stopwords.words('russian') \n",
    "    with open('stopwords-ru.txt', 'r', encoding='utf-8') as f:\n",
    "        external_list = f.read().split('\\n')\n",
    "    final_list = list(set(nltk_list) | set(external_list)) if add_external == True else nltk_list\n",
    "        \n",
    "    return final_list\n",
    "\n",
    "russian_stopwords = get_stopwords_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d3010c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_keyphrases(text, model, n_words):\n",
    "    if model == 'RAKE':\n",
    "        rake.extract_keywords_from_text(text)\n",
    "        keyphrases = rake.get_ranked_phrases()\n",
    "        \n",
    "    if model == 'YAKE':\n",
    "        keyphrases = list(map(lambda x: x[0], yake_model.extract_keywords(text)))\n",
    "        \n",
    "    if model == 'TextRank':\n",
    "        cleaned_text = ' '.join([word.replace('\\n', '') for word in text.split(' ') if word.lower() not in russian_stopwords])\n",
    "        keyphrases = keywords.keywords(text, language = \"russian\").split(\"\\n\")\n",
    "        \n",
    "    if model == 'BERT_spacy':\n",
    "        vectorizer = KeyphraseCountVectorizer(spacy_pipeline=\"ru_core_news_sm\",\n",
    "                                              stop_words=russian_stopwords,\n",
    "                                              pos_pattern='<ADJ.*>*<N.*>+')\n",
    "        bert_keywords = bert_kw.extract_keywords(docs=text,\n",
    "                                                  vectorizer=vectorizer,\n",
    "                                                  nr_candidates=20,\n",
    "                                                  top_n=10,\n",
    "                                                  use_mmr=True,\n",
    "                                                  diversity=0.3,\n",
    "                                                 )\n",
    "        keyphrases = list(map(lambda x: x[0], bert_keywords))\n",
    "        \n",
    "    if model == 'BERT_sklearn':\n",
    "        bert_keywords = bert_kw.extract_keywords(docs=text,\n",
    "                                                  keyphrase_ngram_range=(1,3),\n",
    "                                                  stop_words=russian_stopwords,\n",
    "                                                  nr_candidates=20,\n",
    "                                                  top_n=10,\n",
    "                                                  use_mmr=True,\n",
    "                                                  diversity=0.3)\n",
    "        keyphrases = list(map(lambda x: x[0], bert_keywords))\n",
    "        \n",
    "    if model == 'YaKeyBert':\n",
    "        keyphrases = YaKeyBert_PIPE(text, n_words)\n",
    "    \n",
    "    return keyphrases[:n_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e23d0f",
   "metadata": {},
   "source": [
    "### Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e76e23ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from rake_nltk import Rake\n",
    "\n",
    "rake = Rake(stopwords=russian_stopwords, max_length=3)\n",
    "df_exp_short['keyw_RAKE'] = df_exp_short['name_anot'].apply(lambda x: predict_keyphrases(x, 'RAKE', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c052cc7d",
   "metadata": {},
   "source": [
    "### Yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4420c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import yake\n",
    "\n",
    "yake_model = yake.KeywordExtractor (\n",
    "    lan = \"ru\",     # язык\n",
    "    n = 3,          # максимальное количество слов в фразе\n",
    "    dedupLim = 0.3, # порог похожести слов\n",
    "    top = 10        # количество ключевых слов\n",
    ")\n",
    "\n",
    "df_exp_short['keyw_YAKE'] = df_exp_short['name_anot'].apply(lambda x: predict_keyphrases(x, 'YAKE', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef38125",
   "metadata": {},
   "source": [
    "### TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df151c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from summa import keywords\n",
    "\n",
    "df_exp_short['keyw_TextRank'] = df_exp_short['name_anot'].apply(lambda x: predict_keyphrases(x, 'TextRank', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf433b1",
   "metadata": {},
   "source": [
    "### KeyBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12a90d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14de216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_kw = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5089ae27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_exp_short['keyw_BERT_spacy'] = df_exp_short['name_anot'].apply(lambda x: predict_keyphrases(x, 'BERT_spacy', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "919b5509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ilya\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['хотел'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_exp_short['keyw_BERT_sklearn'] = df_exp_short['name_anot'].apply(lambda x: predict_keyphrases(x, 'BERT_sklearn', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c1584",
   "metadata": {},
   "source": [
    "### YaKeyBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e66731fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593f79c015d440ada17e10f34003ddfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1eb4c386754bea8b6bdaf8515d9433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40b817488fb4d2a8dbe8d6136248a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7bd349bd7a4d92bf9d9c2196540de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yake_model = yake.KeywordExtractor (\n",
    "    lan = \"ru\",     # язык\n",
    "    n = 4,          # максимальное количество слов в фразе\n",
    "    dedupLim = 0.3, # порог похожести слов\n",
    "    top = 10        # количество ключевых слов\n",
    ")\n",
    "\n",
    "vectorizer = KeyphraseCountVectorizer(spacy_pipeline=\"ru_core_news_sm\",\n",
    "                                      stop_words=russian_stopwords,\n",
    "                                      pos_pattern='<ADJ.*>*<N.*>+')\n",
    "hf_model = pipeline(\"feature-extraction\", model='xlm-roberta-base')\n",
    "bert_kw = KeyBERT(model=hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afa021e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def YaKeyBert_PIPE(text, n_keywords):\n",
    "    bert_keywords = []\n",
    "    \n",
    "    yake_keyphrases = list(map(lambda x: x[0], yake_model.extract_keywords(text)))\n",
    "    for phrase in yake_keyphrases:\n",
    "        try:\n",
    "            phrase_keywords = bert_kw.extract_keywords(\n",
    "                                    docs=phrase,\n",
    "                                    vectorizer=vectorizer,\n",
    "                                    nr_candidates=20,\n",
    "                                    top_n=1,\n",
    "                                    use_mmr=True,\n",
    "                                    diversity=0.3,)[0][0]\n",
    "            n_words = len(phrase_keywords.split(' ')) \n",
    "            if n_words >= 1 and n_words <= 4:\n",
    "                bert_keywords.append(phrase_keywords)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    keywords = list(set(bert_keywords))[:n_keywords]\n",
    "        \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a0b0e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_exp_short['keyw_yakeybert_anot_10'] = df_exp_short['anot'].apply(lambda x: predict_keyphrases(x, 'YaKeyBert', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d47214fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_exp_short['keyw_yakeybert_name_anot_10'] = df_exp_short['name_anot'].apply(lambda x: predict_keyphrases(x, 'YaKeyBert', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ce162",
   "metadata": {},
   "source": [
    "## Оценка качества моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb48831d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def stem_phrase(phrase):\n",
    "    stemmer = SnowballStemmer(\"russian\")  \n",
    "    words = phrase.lower().split(' ')\n",
    "    stemmed_phrase = ' '.join([stemmer.stem(word) for word in words])\n",
    "    \n",
    "    return stemmed_phrase\n",
    "\n",
    "def check_occurence(check_phrase, phrases_list):\n",
    "    for phrase in phrases_list:\n",
    "        k = 0\n",
    "        phrase_granulated = set(phrase.split(' '))\n",
    "        check_phrase_granulated = check_phrase.split(' ')\n",
    "        for check_word in check_phrase_granulated:\n",
    "            if check_word in phrase_granulated:\n",
    "                k += 1\n",
    "        if k/len(check_phrase_granulated) > 0.5:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "    \n",
    "def keywords_precision_score(keyw_true, keyw_pred, k=5):\n",
    "    if len(keyw_true) == 0 or len(keyw_pred) == 0:\n",
    "        return 0\n",
    "        \n",
    "    n = 0\n",
    "    keyw_true = [stem_phrase(phrase) for phrase in keyw_true]\n",
    "    keyw_pred = [stem_phrase(phrase) for phrase in keyw_pred[:k]]\n",
    "        \n",
    "    for keyw in keyw_pred:\n",
    "        if check_occurence(keyw, keyw_true): \n",
    "            n+=1\n",
    "    \n",
    "    return n/k\n",
    "\n",
    "def keywords_recall_score(keyw_true, keyw_pred, k=5):\n",
    "    if len(keyw_true) == 0 or len(keyw_pred) == 0:\n",
    "        return 0\n",
    "    \n",
    "    n_match = 0\n",
    "    n_true = len(keyw_true)\n",
    "    keyw_true = [stem_phrase(phrase) for phrase in keyw_true]\n",
    "    keyw_pred = [stem_phrase(phrase) for phrase in keyw_pred[:k]]\n",
    "        \n",
    "    for keyw in keyw_true:\n",
    "        if check_occurence(keyw, keyw_pred): \n",
    "            n_match += 1\n",
    "    \n",
    "    return n_match/n_true\n",
    "\n",
    "def keywords_mean_reciprocal_rank(keyw_true, keyw_pred, k=5):\n",
    "    if len(keyw_true) == 0 or len(keyw_pred) == 0:\n",
    "        return 0\n",
    "    \n",
    "    n = 0\n",
    "    keyw_true = [stem_phrase(phrase) for phrase in keyw_true]\n",
    "    keyw_pred = [stem_phrase(phrase) for phrase in keyw_pred[:k]]\n",
    "        \n",
    "    for n_el, keyw in enumerate(keyw_pred):\n",
    "        if check_occurence(keyw, keyw_true): \n",
    "            n = n_el + 1\n",
    "            break\n",
    "    \n",
    "    return 1/n if n != 0 else 0\n",
    "\n",
    "def keywords_mean_average_precision(keyw_true, keyw_pred, k):\n",
    "    if len(keyw_true) == 0 or len(keyw_pred) == 0:\n",
    "        return 0\n",
    "    \n",
    "    idxs = []\n",
    "    keyw_true = [stem_phrase(phrase) for phrase in keyw_true]\n",
    "    keyw_pred = [stem_phrase(phrase) for phrase in keyw_pred[:k]]\n",
    "        \n",
    "    for n_el, keyw in enumerate(keyw_pred):\n",
    "        if check_occurence(keyw, keyw_true):\n",
    "            idxs.append(n_el + 1)\n",
    "            \n",
    "    if len(idxs) == 0:\n",
    "        return 0\n",
    "    \n",
    "    precisions = [(n_el+1)/ idx for n_el, idx in enumerate(idxs)]\n",
    "    return sum(precisions)/len(precisions)\n",
    "\n",
    "def keywords_IoU(keyw_true, keyw_pred, k=5):\n",
    "    if len(keyw_true) == 0 or len(keyw_pred) == 0:\n",
    "        return 0\n",
    "        \n",
    "    n_intersect = 0\n",
    "    keyw_true = [stem_phrase(phrase) for phrase in keyw_true]\n",
    "    keyw_pred = [stem_phrase(phrase) for phrase in keyw_pred[:k]]\n",
    "    union = len(set(keyw_true + keyw_pred))\n",
    "        \n",
    "    for keyw in keyw_pred:\n",
    "        if check_occurence(keyw, keyw_true): \n",
    "            n_intersect += 1\n",
    "    \n",
    "    return n_intersect/union\n",
    "\n",
    "def keywords_stem(keyw_true, keyw_pred, k=5):\n",
    "    n = 0\n",
    "    keyw_true = [stem_phrase(phrase) for phrase in keyw_true]\n",
    "    keyw_pred = [stem_phrase(phrase) for phrase in keyw_pred[:k]]\n",
    "    \n",
    "    return keyw_true, keyw_pred\n",
    "\n",
    "def evaluate_extractor(y_true, y_pred, metric, k=5):\n",
    "    metric_values = [] \n",
    "    \n",
    "    try:\n",
    "        if metric == 'precision':\n",
    "            for idx in range(len(y_true)):\n",
    "                value = keywords_precision_score(y_true[idx], y_pred[idx], k)\n",
    "                metric_values.append(value)\n",
    "\n",
    "        if metric == 'recall':\n",
    "            for idx in range(len(y_true)):\n",
    "                value = keywords_recall_score(y_true[idx], y_pred[idx], k)\n",
    "                metric_values.append(value)\n",
    "\n",
    "        if metric == 'MRR':\n",
    "            for idx in range(len(y_true)):\n",
    "                value = keywords_mean_reciprocal_rank(y_true[idx], y_pred[idx], k)\n",
    "                metric_values.append(value)\n",
    "\n",
    "        if metric == 'MAP':\n",
    "            for idx in range(len(y_true)):\n",
    "                value = keywords_mean_average_precision(y_true[idx], y_pred[idx], k)\n",
    "                metric_values.append(value)\n",
    "                \n",
    "        if metric == 'IoU':\n",
    "            for idx in range(len(y_true)):\n",
    "                value = keywords_IoU(y_true[idx], y_pred[idx], k)\n",
    "                metric_values.append(value)\n",
    "            \n",
    "        return np.mean(metric_values)\n",
    "    \n",
    "    except KeyError:\n",
    "        print(y_true[idx])\n",
    "        \n",
    "def extraction_evaluation_report(df, keyw_true_col, models, metrics, size=0.1):\n",
    "    result = dict()\n",
    "    df = df.sample(round(size*len(df))).reset_index()\n",
    "    \n",
    "    for model_name in models:\n",
    "        pred_column = 'keyw_' + model_name\n",
    "        metrics_values = dict()\n",
    "        for metric_name in metrics:\n",
    "            metrics_values[metric_name+'_at_5'] = evaluate_extractor(df[keyw_true_col], df[pred_column], metric_name, k=5)\n",
    "            metrics_values[metric_name+'_at_10'] = evaluate_extractor(df[keyw_true_col], df[pred_column], metric_name, k=10)\n",
    "        \n",
    "        result[model_name] = metrics_values\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e1e61a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eval_result = extraction_evaluation_report(df_exp_short,\n",
    "                                           'keywords',\n",
    "                                           ['RAKE', 'YAKE', 'TextRank',\n",
    "                                            'BERT_spacy', 'BERT_sklearn',\n",
    "                                            'yakeybert_name_anot_10', 'yakeybert_anot_10'\n",
    "                                           ],\n",
    "                                           ['precision', 'recall', 'MRR', 'MAP', 'IoU'],\n",
    "                                           size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0786c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor: RAKE\n",
      "\t precision_at_5 : 0.1\n",
      "\t precision_at_10 : 0.074\n",
      "\t recall_at_5 : 0.165\n",
      "\t recall_at_10 : 0.252\n",
      "\t MRR_at_5 : 0.175\n",
      "\t MRR_at_10 : 0.191\n",
      "\t MAP_at_5 : 0.174\n",
      "\t MAP_at_10 : 0.187\n",
      "\t IoU_at_5 : 0.058\n",
      "\t IoU_at_10 : 0.06\n",
      "Extractor: YAKE\n",
      "\t precision_at_5 : 0.244\n",
      "\t precision_at_10 : 0.188\n",
      "\t recall_at_5 : 0.339\n",
      "\t recall_at_10 : 0.412\n",
      "\t MRR_at_5 : 0.395\n",
      "\t MRR_at_10 : 0.421\n",
      "\t MAP_at_5 : 0.397\n",
      "\t MAP_at_10 : 0.39\n",
      "\t IoU_at_5 : 0.131\n",
      "\t IoU_at_10 : 0.133\n",
      "Extractor: TextRank\n",
      "\t precision_at_5 : 0.292\n",
      "\t precision_at_10 : 0.204\n",
      "\t recall_at_5 : 0.103\n",
      "\t recall_at_10 : 0.148\n",
      "\t MRR_at_5 : 0.348\n",
      "\t MRR_at_10 : 0.364\n",
      "\t MAP_at_5 : 0.376\n",
      "\t MAP_at_10 : 0.396\n",
      "\t IoU_at_5 : 0.182\n",
      "\t IoU_at_10 : 0.186\n",
      "Extractor: BERT_spacy\n",
      "\t precision_at_5 : 0.156\n",
      "\t precision_at_10 : 0.124\n",
      "\t recall_at_5 : 0.311\n",
      "\t recall_at_10 : 0.415\n",
      "\t MRR_at_5 : 0.284\n",
      "\t MRR_at_10 : 0.299\n",
      "\t MAP_at_5 : 0.282\n",
      "\t MAP_at_10 : 0.274\n",
      "\t IoU_at_5 : 0.088\n",
      "\t IoU_at_10 : 0.092\n",
      "Extractor: BERT_sklearn\n",
      "\t precision_at_5 : 0.16\n",
      "\t precision_at_10 : 0.128\n",
      "\t recall_at_5 : 0.211\n",
      "\t recall_at_10 : 0.349\n",
      "\t MRR_at_5 : 0.272\n",
      "\t MRR_at_10 : 0.288\n",
      "\t MAP_at_5 : 0.267\n",
      "\t MAP_at_10 : 0.264\n",
      "\t IoU_at_5 : 0.086\n",
      "\t IoU_at_10 : 0.091\n",
      "Extractor: yakeybert_name_anot_10\n",
      "\t precision_at_5 : 0.256\n",
      "\t precision_at_10 : 0.198\n",
      "\t recall_at_5 : 0.253\n",
      "\t recall_at_10 : 0.318\n",
      "\t MRR_at_5 : 0.454\n",
      "\t MRR_at_10 : 0.484\n",
      "\t MAP_at_5 : 0.416\n",
      "\t MAP_at_10 : 0.428\n",
      "\t IoU_at_5 : 0.144\n",
      "\t IoU_at_10 : 0.166\n",
      "Extractor: yakeybert_anot_10\n",
      "\t precision_at_5 : 0.172\n",
      "\t precision_at_10 : 0.132\n",
      "\t recall_at_5 : 0.181\n",
      "\t recall_at_10 : 0.271\n",
      "\t MRR_at_5 : 0.271\n",
      "\t MRR_at_10 : 0.289\n",
      "\t MAP_at_5 : 0.266\n",
      "\t MAP_at_10 : 0.263\n",
      "\t IoU_at_5 : 0.098\n",
      "\t IoU_at_10 : 0.112\n"
     ]
    }
   ],
   "source": [
    "for model in eval_result.keys():\n",
    "    print(\"Extractor:\", model)\n",
    "    for metric in eval_result[model].keys():\n",
    "        print('\\t', metric, \":\", round(eval_result[model][metric], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666024f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
